{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3172,
     "status": "ok",
     "timestamp": 1745256334992,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "1GoXYSawYg8i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1745256344531,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "cFT8P-SHYiR5"
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745256354830,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "xk2qPoOmYotT"
   },
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    counts = Counter(y)\n",
    "    return 1 - sum((count / len(y)) ** 2 for count in counts.values())\n",
    "\n",
    "# Split the dataset based on feature and threshold\n",
    "def split_dataset(X, y, feature_index, threshold):\n",
    "    left = X[:, feature_index] <= threshold\n",
    "    right = X[:, feature_index] > threshold\n",
    "    return X[left], y[left], X[right], y[right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1745256370355,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "MkV_mKH2Ysek"
   },
   "outputs": [],
   "source": [
    "def best_split(X, y):\n",
    "    best_gini = float('inf')\n",
    "    best_index = None\n",
    "    best_threshold = None\n",
    "\n",
    "    for feature_index in range(X.shape[1]):\n",
    "        thresholds = np.unique(X[:, feature_index])\n",
    "        for threshold in thresholds:\n",
    "            X_left, y_left, X_right, y_right = split_dataset(X, y, feature_index, threshold)\n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "\n",
    "            gini_left = gini(y_left)\n",
    "            gini_right = gini(y_right)\n",
    "            weighted_avg = (len(y_left) * gini_left + len(y_right) * gini_right) / len(y)\n",
    "\n",
    "            if weighted_avg < best_gini:\n",
    "                best_gini = weighted_avg\n",
    "                best_index = feature_index\n",
    "                best_threshold = threshold\n",
    "\n",
    "    return best_index, best_threshold\n",
    "\n",
    "# Node class for tree\n",
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1745256623657,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "1vtzx6TDZn8X"
   },
   "outputs": [],
   "source": [
    "def build_tree(X, y, depth=0, max_depth=3):\n",
    "    if len(set(y)) == 1:\n",
    "        return Node(value=y[0])\n",
    "    if depth >= max_depth or len(y) == 0:\n",
    "        return Node(value=Counter(y).most_common(1)[0][0])\n",
    "\n",
    "    feature_index, threshold = best_split(X, y)\n",
    "    if feature_index is None:\n",
    "        return Node(value=Counter(y).most_common(1)[0][0])\n",
    "\n",
    "    X_left, y_left, X_right, y_right = split_dataset(X, y, feature_index, threshold)\n",
    "    left = build_tree(X_left, y_left, depth + 1, max_depth)\n",
    "    right = build_tree(X_right, y_right, depth + 1, max_depth)\n",
    "    return Node(feature_index, threshold, left, right)\n",
    "\n",
    "# Prediction\n",
    "def predict_single(node, x):\n",
    "    while not node.is_leaf():\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            node = node.left\n",
    "        else:\n",
    "            node = node.right\n",
    "    return node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745256632524,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "C77uf6X8ZrVb",
    "outputId": "ed4a11f5-d33b-41da-c52c-1bc420cb36da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "def predict(node, X):\n",
    "    return np.array([predict_single(node, sample) for sample in X])\n",
    "\n",
    "# Train the tree\n",
    "tree = build_tree(X_train, y_train, max_depth=3)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = predict(tree, X_test)\n",
    "\n",
    "# Evaluate\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1745257117696,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "CImlkD5qfrEK",
    "outputId": "7ff66b45-e8a2-4937-94a9-74a42c2787dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('Iris.csv')\n",
    "print(data.head())\n",
    "\n",
    "X = data.drop('Species', axis=1)\n",
    "y = data['Species']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F29AUj8EnxFK"
   },
   "source": [
    "# **Decision Tree Classifier**\n",
    "Theory: A Decision Tree is a non-linear supervised learning algorithm used for both classification and regression tasks. It divides the data into smaller subsets based on feature values, creating a tree-like structure where:\n",
    "\n",
    "Nodes represent features.\n",
    "Edges represent feature values.\n",
    "Leaves represent the target variableâ€™s class label or a predicted value.\n",
    "In this case, the algorithm builds a tree to predict the Species of a flower based on features like SepalLengthCm, SepalWidthCm, PetalLengthCm, and PetalWidthCm.\n",
    "\n",
    "Steps Involved:\n",
    "\n",
    "The algorithm selects the feature that best splits the data (using metrics like Gini Impurity or Entropy) and recursively creates nodes until it reaches a stopping criterion (e.g., maximum depth or minimum samples per leaf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1745257122783,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "D8XsIv8SlZ0T",
    "outputId": "f26a534e-80b4-41da-c2e8-90240b4ab904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.9619047619047619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Check the model's performance\n",
    "train_accuracy = dt.score(X_train, y_train)\n",
    "test_accuracy = dt.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehu4IUO-n5M6"
   },
   "source": [
    "**Cost Complexity Pruning**\n",
    "\n",
    "Theory: Cost Complexity Pruning (also known as \"Weakest Link Pruning\") is a technique used to reduce overfitting in decision trees by pruning branches that provide little predictive power.\n",
    "\n",
    "Pruning removes branches that have little importance and simplifies the model, preventing it from fitting to noise in the data. The pruning is controlled by a parameter ccp_alpha. By tuning this parameter, the tree's complexity can be controlled:\n",
    "\n",
    "Low values of ccp_alpha result in a tree that closely fits the training data (possibly overfitting).\n",
    "High values of ccp_alpha lead to a simpler tree with fewer splits (higher bias but lower variance).\n",
    "Pruning helps balance the trade-off between bias and variance, leading to better generalization on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1745257135742,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "LVeBLQDalluT",
    "outputId": "e5481da3-bf90-49cc-8b7f-c0f69050e716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeClassifier(ccp_alpha=np.float64(0.0), random_state=42), DecisionTreeClassifier(ccp_alpha=np.float64(0.3111111111111111),\n",
      "                       random_state=42), DecisionTreeClassifier(ccp_alpha=np.float64(0.35259259259259257),\n",
      "                       random_state=42)]\n",
      "Best Pruned Tree Test Accuracy: 0.9809523809523809\n"
     ]
    }
   ],
   "source": [
    "path = dt.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "pruned_trees = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    pruned_tree = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    pruned_tree.fit(X_train, y_train)\n",
    "    pruned_trees.append(pruned_tree)\n",
    "\n",
    "print(pruned_trees)\n",
    "test_accuracies = [tree.score(X_test, y_test) for tree in pruned_trees]\n",
    "best_tree = pruned_trees[test_accuracies.index(max(test_accuracies))]\n",
    "\n",
    "print(f\"Best Pruned Tree Test Accuracy: {max(test_accuracies)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpcXeQHpoHFC"
   },
   "source": [
    "# **Random Forest Classifier**\n",
    "\n",
    "Theory: A Random Forest is an ensemble learning method that creates multiple decision trees and combines their results to improve performance and reduce overfitting.\n",
    "\n",
    "Ensemble Learning: The idea is to build multiple models (here, decision trees) and combine them to get a stronger, more accurate model.\n",
    "\n",
    "Bagging (Bootstrap Aggregating): In Random Forest, each tree is trained on a random subset of the data using bootstrapping, meaning that the data points are sampled with replacement. Each tree may be trained on different subsets, making the model robust to overfitting.\n",
    "\n",
    "Feature Randomness: For each split in a tree, a random subset of features is considered, ensuring that the trees are decorrelated and reducing overfitting.\n",
    "The predictions from all the trees are averaged for regression or voted on for classification to determine the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1742291076883,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "NvoO444SmFlD",
    "outputId": "99e3a80b-4932-4bde-e274-8d8f822d0b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy: 1.0\n",
      "Random Forest Testing Accuracy: 0.9904761904761905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy_rf = rf.score(X_train, y_train)\n",
    "test_accuracy_rf = rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Random Forest Training Accuracy: {train_accuracy_rf}\")\n",
    "print(f\"Random Forest Testing Accuracy: {test_accuracy_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFVeWuhuoXQ5"
   },
   "source": [
    "# **AdaBoost with Decision Stumps**\n",
    "Theory: AdaBoost (Adaptive Boosting) is an ensemble learning technique that combines weak learners (models that perform slightly better than random guessing, such as decision stumps) into a strong classifier.\n",
    "\n",
    "\n",
    "Decision Stumps are shallow decision trees with only one split (a depth of 1). While a single decision stump is weak, AdaBoost can iteratively improve its performance by focusing on the mistakes made by previous stumps.\n",
    "Boosting refers to training models sequentially, where each subsequent model focuses on the errors made by the previous ones. The algorithm assigns higher weights to misclassified data points so that the next model focuses more on correcting those errors.\n",
    "\n",
    "Working of AdaBoost:\n",
    "\n",
    "Initially, all data points are given equal weight.\n",
    "After each weak learner (decision stump) is trained, the weights of the misclassified data points are increased, and those of correctly classified points are decreased.\n",
    "A final strong model is formed by combining the weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1742291078647,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "PuGIPARnnIXJ",
    "outputId": "0eb488cd-3c2f-42d3-e8af-9789820b6a1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Training Accuracy: 1.0\n",
      "AdaBoost Testing Accuracy: 0.9809523809523809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "decision_stump = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "ada_boost = AdaBoostClassifier(estimator=decision_stump, n_estimators=50, random_state=42)\n",
    "\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy_ada = ada_boost.score(X_train, y_train)\n",
    "test_accuracy_ada = ada_boost.score(X_test, y_test)\n",
    "\n",
    "print(f\"AdaBoost Training Accuracy: {train_accuracy_ada}\")\n",
    "print(f\"AdaBoost Testing Accuracy: {test_accuracy_ada}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB5stI3qolIC"
   },
   "source": [
    "# **Model Evaluation**\n",
    "Theory: Once the models are trained, they need to be evaluated to ensure that they generalize well to unseen data. Common evaluation metrics include:\n",
    "\n",
    "Accuracy: The ratio of correctly predicted instances to the total instances. Itâ€™s a general measure of model performance but may not be informative for imbalanced datasets.\n",
    "Confusion Matrix: A table used to describe the performance of a classification model. It shows the number of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "Precision, Recall, F1-Score: These metrics are especially useful in imbalanced classification problems.\n",
    "Precision: The ratio of true positive predictions to all positive predictions.\n",
    "Recall: The ratio of true positive predictions to all actual positives.\n",
    "F1-Score: The harmonic mean of precision and recall, used when you need a balance between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1742291082619,
     "user": {
      "displayName": "SWAYAM CHANDAK",
      "userId": "12997756181521946106"
     },
     "user_tz": -330
    },
    "id": "5tEMlOWunLAZ",
    "outputId": "761ad256-faff-465b-d77b-49bd9c505cfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.97      1.00      0.99        36\n",
      "           2       1.00      0.97      0.99        36\n",
      "\n",
      "    accuracy                           0.99       105\n",
      "   macro avg       0.99      0.99      0.99       105\n",
      "weighted avg       0.99      0.99      0.99       105\n",
      "\n",
      "Confusion Matrix (Random Forest):\n",
      "[[33  0  0]\n",
      " [ 0 36  0]\n",
      " [ 0  1 35]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Classification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Confusion Matrix (Random Forest):\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9vta3MtG_DE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1E8A42UIWJpK56cExolOhHHdY4LQ9RTS9",
     "timestamp": 1742275973864
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
